# -*- coding: utf-8 -*-
"""faster-r-cnn-inference-shit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aGhAHQOp-G8w_LJ0mOtcrWdHOEsha6Q3
"""

def convert_to_yolo_format(box, img_width, img_height):
    """
    Converts a bounding box from (x1, y1, x2, y2) to YOLO format:
    (x_center, y_center, width, height), all normalized [0,1].

    Parameters:
    - box: tuple/list (x1, y1, x2, y2)
    - img_width: width of the image
    - img_height: height of the image

    Returns:
    - tuple: (x_center, y_center, width, height) in YOLO format
    """
    label, x1, y1, x2, y2 = box

    # Absolute values
    box_width = x2 - x1
    box_height = y2 - y1
    x_center = x1 + box_width / 2
    y_center = y1 + box_height / 2

    # Normalize
    x_center /= img_width
    y_center /= img_height
    box_width /= img_width
    box_height /= img_height

    return f"{label}, {x_center}, {y_center}, {box_width}, {box_height}"

from PIL import Image
from torchvision import transforms
import torch
from torchvision.ops import nms
import matplotlib.pyplot as plt
import matplotlib.patches as patches

def predict_and_visualize(model, image_path, device, num_classes=1, score_threshold=0.5, iou_threshold=0.3):
    """
    Predict and visualize detections on a single image.

    Args:
        model: Pretrained detection model
        image_path: Path to the input image
        device: 'cuda' or 'cpu'
        num_classes: Number of classes (default=1)
        score_threshold: Minimum confidence score (default=0.5)
        iou_threshold: NMS threshold (default=0.3)
    """
    # 1. Load and preprocess image
    img = Image.open(image_path).convert("RGB")
    transform = transforms.Compose([transforms.ToTensor()])
    img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension

    # 2. Run prediction
    model.to(device)
    model.eval()

    with torch.no_grad():
        output = model(img_tensor)[0]  # Remove batch dimension

    # 3. Filter predictions
    boxes = output['boxes'].cpu()
    scores = output['scores'].cpu()
    labels = output['labels'].cpu()

    # Apply score threshold
    keep_score = scores > score_threshold
    boxes = boxes[keep_score]
    scores = scores[keep_score]
    labels = labels[keep_score]

    # Apply NMS
    if boxes.shape[0] > 0:
        keep_nms = nms(boxes, scores, iou_threshold)
        boxes = boxes[keep_nms]
        scores = scores[keep_nms]
        labels = labels[keep_nms]

    # 4. Visualization
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))
    ax.imshow(img)

    # Plot predicted boxes
    for box, score, label in zip(boxes, scores, labels):
        x1, y1, x2, y2 = box
        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1,
                                linewidth=2, edgecolor='red', facecolor='none')
        ax.add_patch(rect)
        ax.text(x1, y1 - 5, f'Class {label.item()} ({score:.2f})',
                color='red', bbox=dict(facecolor='white', alpha=0.7))

    plt.axis('off')
    plt.show()

    selected_box = None

    # 2. Find the index of the box with the highest score
    if len(boxes) > 1:
        max_score_idx = torch.argmax(scores).item()
        best_box = boxes[max_score_idx]
        best_score = scores[max_score_idx]
        best_label = labels[max_score_idx]

        # 3. Extract coordinates
        x1, y1, x2, y2 = best_box.tolist()  # Convert to Python list
        selected_box = (best_label, x1, y1, x2, y2)

        print(f"Highest-scoring box: {[x1, y1, x2, y2]}, Score: {best_score:.2f}")
    elif len(boxes) == 1:
        x1, y1, x2, y2 = boxes[0].tolist()  # Convert to Python list
        selected_box = (labels[0], x1, y1, x2, y2)
    else:
        print("No boxes detected after NMS")


    # Print summary
    print(f"Detected {len(boxes)} objects:")
    for i, (box, score, label) in enumerate(zip(boxes, scores, labels)):
        print(f"Object {i+1}: Class {label.item()} | Confidence: {score:.2f} | Box: {box.tolist()}")
    # Convert to YOLO format if box exists
        if selected_box is not None:
            label, x1, y1, x2, y2 = selected_box
            return convert_to_yolo_format((label, x1, y1, x2, y2), img_width=640, img_height=640)

    print("No boxes detected")
    return None

import torchvision
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.models.detection import FasterRCNN

def get_model(num_classes):
    # Load a pre-trained ResNet18 model
    backbone = torchvision.models.resnet18(pretrained=True)

    # Remove the last fully connected layer and average pooling
    backbone = torch.nn.Sequential(*list(backbone.children())[:-2])

    # We need the output channels of the backbone for the anchor generator
    backbone.out_channels = 512

    # Anchor generator
    anchor_generator = AnchorGenerator(
        sizes=((32, 64, 128, 256, 512),),
        aspect_ratios=((0.5, 1.0, 2.0),)
    )

    # Feature map names for the RoI heads
    roi_pooler = torchvision.ops.MultiScaleRoIAlign(
        featmap_names=['0'],
        output_size=7,
        sampling_ratio=2
    )

    # Put the pieces together in the Faster R-CNN model
    model = FasterRCNN(
        backbone,
        num_classes=num_classes,
        rpn_anchor_generator=anchor_generator,
        box_roi_pool=roi_pooler
    )

    return model

from google.colab import files
import os
from google.colab import drive
drive.mount('/content/drive')

if __name__ == "__main__":
    # Set device
    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

    model_path = "fasterrcnn_resnet18.pth"

    if not os.path.exists(model_path):
        print("Model file not found. Please upload it.")
        uploaded = files.upload()  # open upload dialog
    else:
        print("Model file already exists. Skipping upload.")

    # Create model structure (same as used during training)
    model = get_model(num_classes=2)  # Assuming 2 classes: background + your object
    model.load_state_dict(torch.load("fasterrcnn_resnet18.pth", map_location=device))

    # Example usage:
    final_prediction = predict_and_visualize(model, "/content/drive/MyDrive/Shark/Screenshots/screenshot_0034.png", device=device, num_classes=2)
    print(f"Final prediction: {final_prediction}")